{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUEwumOlrUXT",
    "outputId": "0135b1cd-aec8-42b5-f165-c9bcdde429ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mdQGZO36TGdD",
    "outputId": "befa4c2c-5f45-42ed-ef28-787d8c909836"
   },
   "outputs": [],
   "source": [
    "!mkdir my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BhIWnn51Tymi",
    "outputId": "9e42412e-ebf2-4cc1-ccb6-0001213b23b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "지정된 경로를 찾을 수 없습니다.\n",
      "curl: (3) URL using bad/illegal format or missing URL\n"
     ]
    }
   ],
   "source": [
    "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" > /dev/null\n",
    "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" -o my_data/wiki_20190620_small.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dI2uOKvyT6jK",
    "outputId": "44f311eb-2561-4a24-fc7a-d351da8f3431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Dln3ZL-e6HD2"
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "HNMUWRoM5j6a",
    "outputId": "785fe9a6-11bd-4aa0-e78a-c10c9b58a30d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'4.15.0'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0G8TD17pT89K",
    "outputId": "ac9fa96e-0e70-46ee-995d-9d72d22fa193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘wordPieceTokenizer’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir wordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xk8PNLHIUA4h",
    "outputId": "bbb77aad-3bff-4bca-e767-d760954fb21c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wordPieceTokenizer/my_tokenizer-vocab.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# Initialize an empty tokenizer\n",
    "wp_tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,   # [\"이순신\", \"##은\", \" \", \"조선\"] ->  [\"이순신\", \"##은\", \"조선\"]\n",
    "    # if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "    handle_chinese_chars=True,  # 한자는 모두 char 단위로 쪼개버립니다.\n",
    "    strip_accents=False,    # True: [YehHamza] -> [Yep, Hamza]\n",
    "    lowercase=False,    # Hello -> hello\n",
    ")\n",
    "\n",
    "# And then train\n",
    "wp_tokenizer.train(\n",
    "    files=\"my_data/wiki_20190620_small.txt\",\n",
    "    vocab_size=20000,   # vocab size 를 지정해줄 수 있습니다.\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    wordpieces_prefix=\"##\"\n",
    ")\n",
    "\n",
    "# Save the files\n",
    "wp_tokenizer.save_model(\"wordPieceTokenizer\", \"my_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3u3_XBzFUrSy",
    "outputId": "fb819f89-1a73-4fc8-f330-a8c6812cecb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(wp_tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3v7wsh9tUtSC",
    "outputId": "fa0986fc-5ec1-46f5-e1f2-3624316fd544"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '##순', '##신은', '[MASK]', '중', '##기의', '무신', '##이다', '.']\n",
      "[705, 1304, 7573, 4, 754, 2602, 13158, 1895, 16]\n"
     ]
    }
   ],
   "source": [
    "text = \"이순신은 [MASK] 중기의 무신이다.\"\n",
    "tokenized_text = wp_tokenizer.encode(text)\n",
    "print(tokenized_text.tokens)\n",
    "print(tokenized_text.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXQvXY7jUvMD",
    "outputId": "4fba4a49-f990-49c3-8e9d-3aea5ddad0b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "a75xM9WsUwRi"
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForPreTraining, BertTokenizerFast #bertformaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "oDxZLnCLUyNa"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast(\n",
    "    vocab_file='/content/wordPieceTokenizer/my_tokenizer-vocab.txt',\n",
    "    max_len=128,\n",
    "    do_lower_case=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HkBRuXpQU1qr",
    "outputId": "8e092d46-8b07-4eea-f191-aa63dd36b6e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['은', '[', 'M', '##AS', '##K', ']', '조선', '중', '##기의', '무신', '##이다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"은 [MASK] 조선 중기의 무신이다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQwE426x4NAv",
    "outputId": "077eb008-4474-4802-8e5c-b3070660a815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '##순', '##신은', '[MASK]', '중', '##기의', '무신', '##이다', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'mask_token':'[MASK]'})\n",
    "print(tokenizer.tokenize(\"이순신은 [MASK] 중기의 무신이다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43HGV14nU3fq",
    "outputId": "fd342419-4241-45e8-ab98-d6fe730053c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101720098"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig(    # https://huggingface.co/transformers/model_doc/bert.html#bertconfig\n",
    "    vocab_size=20000, # default는 영어 기준이므로 내가 만든 vocab size에 맞게 수정해줘야 함\n",
    "    # hidden_size=512,\n",
    "    # num_hidden_layers=12,    # layer num\n",
    "    # num_attention_heads=8,    # transformer attention head number\n",
    "    # intermediate_size=3072,   # transformer 내에 있는 feed-forward network의 dimension size\n",
    "    # hidden_act=\"gelu\",\n",
    "    # hidden_dropout_prob=0.1,\n",
    "    # attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=128,    # embedding size 최대 몇 token까지 input으로 사용할 것인지 지정\n",
    "    # type_vocab_size=2,    # token type ids의 범위 (BERT는 segmentA, segmentB로 2종류)\n",
    "    # pad_token_id=0,\n",
    "    # position_embedding_type=\"absolute\"\n",
    ")\n",
    "\n",
    "model = BertForPreTraining(config=config)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "xMHD1mHvW9tl"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "MLqabAUgXPoF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from typing import Dict, List, Optional\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from filelock import FileLock\n",
    "\n",
    "from transformers.utils import logging\n",
    "\n",
    "logger = logging.get_logger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "rgsqrTrOuJiX"
   },
   "outputs": [],
   "source": [
    "class TextDatasetForNextSentencePrediction(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach soon.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        file_path: str,\n",
    "        block_size: int,\n",
    "        overwrite_cache=False,\n",
    "        short_seq_probability=0.1,\n",
    "        nsp_probability=0.5,\n",
    "    ):\n",
    "        # 여기 부분은 학습 데이터를 caching하는 부분입니다 :-)\n",
    "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
    "\n",
    "        self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=True)\n",
    "        self.short_seq_probability = short_seq_probability\n",
    "        self.nsp_probability = nsp_probability\n",
    "\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            directory,\n",
    "            \"cached_nsp_{}_{}_{}\".format(\n",
    "                tokenizer.__class__.__name__,\n",
    "                str(block_size),\n",
    "                filename,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "\n",
    "        # Input file format:\n",
    "        # (1) One sentence per line. These should ideally be actual sentences, not\n",
    "        # entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "        # sentence boundaries for the \"next sentence prediction\" task).\n",
    "        # (2) Blank lines between documents. Document boundaries are needed so\n",
    "        # that the \"next sentence prediction\" task doesn't span between documents.\n",
    "        #\n",
    "        # Example:\n",
    "        # I am very happy.\n",
    "        # Here is the second sentence.\n",
    "        #\n",
    "        # A new document.\n",
    "\n",
    "        with FileLock(lock_path):\n",
    "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    self.examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "            else: # 캐시가 없는 경우\n",
    "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
    "                # 여기서부터 본격적으로 dataset을 만듭니다.\n",
    "                self.documents = [[]] # document 단위로 학습이 이뤄짐\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    while True: # 일단 문장을 읽고\n",
    "                        line = f.readline()\n",
    "                        if not line:\n",
    "                            break\n",
    "                        line = line.strip() # 필수!!!!\n",
    "\n",
    "                        # 이중 띄어쓰기가 발견된다면, 나왔던 문장들을 모아 하나의 문서로 묶어버립니다.\n",
    "                        # 즉, 문단 단위로 데이터를 저장합니다.\n",
    "                        if not line and len(self.documents[-1]) != 0:\n",
    "                            self.documents.append([])\n",
    "                        tokens = tokenizer.tokenize(line)\n",
    "                        tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                        if tokens:\n",
    "                            self.documents[-1].append(tokens)\n",
    "                # 이제 코퍼스 전체를 읽고, 문서 데이터를 생성했습니다! :-)\n",
    "                logger.info(f\"Creating examples from {len(self.documents)} documents.\")\n",
    "                self.examples = []\n",
    "                # 본격적으로 학습을 위한 데이터로 변형시켜볼까요?\n",
    "                for doc_index, document in enumerate(self.documents):\n",
    "                    self.create_examples_from_document(document, doc_index) # 함수로 가봅시다.\n",
    "\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def create_examples_from_document(self, document: List[List[int]], doc_index: int):\n",
    "        \"\"\"Creates examples for a single document.\"\"\"\n",
    "        # 문장의 앞, 뒤에 [CLS], [SEP] token이 부착되기 때문에, 내가 지정한 size에서 2 만큼 빼줍니다.\n",
    "        # 예를 들어 128 token 만큼만 학습 가능한 model을 선언했다면, 학습 데이터로부터는 최대 126 token만 가져오게 됩니다.\n",
    "        max_num_tokens = self.block_size - self.tokenizer.num_special_tokens_to_add(pair=True)\n",
    "\n",
    "        # We *usually* want to fill up the entire sequence since we are padding\n",
    "        # to `block_size` anyways, so short sequences are generally wasted\n",
    "        # computation. However, we *sometimes*\n",
    "        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "        # sequences to minimize the mismatch between pretraining and fine-tuning.\n",
    "        # The `target_seq_length` is just a rough target however, whereas\n",
    "        # `block_size` is a hard limit.\n",
    "\n",
    "        # 여기가 재밌는 부분인데요!\n",
    "        # 위에서 설명했듯이, 학습 데이터는 126 token(128-2)을 채워서 만들어지는게 목적입니다.\n",
    "        # 하지만 나중에 BERT를 사용할 때, 126 token 이내의 짧은 문장을 테스트하는 경우도 분명 많을 것입니다 :-)\n",
    "        # 그래서 short_seq_probability 만큼의 데이터에서는 2-126 사이의 random 값으로 학습 데이터를 만들게 됩니다.\n",
    "        target_seq_length = max_num_tokens\n",
    "        if random.random() < self.short_seq_probability:\n",
    "            target_seq_length = random.randint(2, max_num_tokens)\n",
    "\n",
    "        current_chunk = []  # a buffer stored current working segments\n",
    "        current_length = 0\n",
    "        i = 0\n",
    "\n",
    "        # 데이터 구축의 단위는 document 입니다\n",
    "        # 이 때, 무조건 문장_1[SEP]문장_2 이렇게 만들어지는 것이 아니라,\n",
    "        # 126 token을 꽉 채울 수 있게 문장_1+문장_2[SEP]문장_3+문장_4 형태로 만들어질 수 있습니다.\n",
    "        while i < len(document):\n",
    "            segment = document[i]\n",
    "            current_chunk.append(segment)\n",
    "            current_length += len(segment)\n",
    "            if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "                if current_chunk:\n",
    "                    # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                    # (first) sentence.\n",
    "                    a_end = 1\n",
    "                    # 여기서 문장_1+문장_2 가 이루어졌을 때, 길이를 random하게 짤라버립니다 :-)\n",
    "                    if len(current_chunk) >= 2:\n",
    "                        a_end = random.randint(1, len(current_chunk) - 1)\n",
    "                    tokens_a = []\n",
    "                    for j in range(a_end):\n",
    "                        tokens_a.extend(current_chunk[j])\n",
    "                    # 이제 [SEP] 뒷 부분인 segmentB를 살펴볼까요?\n",
    "                    tokens_b = []\n",
    "                    # 50%의 확률로 랜덤하게 다른 문장을 선택하거나, 다음 문장을 학습데이터로 만듭니다.\n",
    "                    if len(current_chunk) == 1 or random.random() < self.nsp_probability:\n",
    "                        is_random_next = True\n",
    "                        target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "                        # This should rarely go for more than one iteration for large\n",
    "                        # corpora. However, just to be careful, we try to make sure that\n",
    "                        # the random document is not the same as the document\n",
    "                        # we're processing.\n",
    "                        for _ in range(10):\n",
    "                            random_document_index = random.randint(0, len(self.documents) - 1)\n",
    "                            if random_document_index != doc_index:\n",
    "                                break\n",
    "                        # 여기서 랜덤하게 선택합니다 :-)\n",
    "                        random_document = self.documents[random_document_index]\n",
    "                        random_start = random.randint(0, len(random_document) - 1)\n",
    "                        for j in range(random_start, len(random_document)):\n",
    "                            tokens_b.extend(random_document[j])\n",
    "                            if len(tokens_b) >= target_b_length:\n",
    "                                break\n",
    "                        # We didn't actually use these segments so we \"put them back\" so\n",
    "                        # they don't go to waste.\n",
    "                        num_unused_segments = len(current_chunk) - a_end\n",
    "                        i -= num_unused_segments\n",
    "                    # Actual next\n",
    "                    else:\n",
    "                        is_random_next = False\n",
    "                        for j in range(a_end, len(current_chunk)):\n",
    "                            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                    # 이제 126 token을 넘는다면 truncation을 해야합니다.\n",
    "                    # 이 때, 126 token 이내로 들어온다면 행위를 멈추고,\n",
    "                    # 만약 126 token을 넘는다면, segmentA와 segmentB에서 랜덤하게 하나씩 제거합니다.\n",
    "                    def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n",
    "                        \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "                        while True:\n",
    "                            total_length = len(tokens_a) + len(tokens_b)\n",
    "                            if total_length <= max_num_tokens:\n",
    "                                break\n",
    "                            trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "                            assert len(trunc_tokens) >= 1\n",
    "                            # We want to sometimes truncate from the front and sometimes from the\n",
    "                            # back to add more randomness and avoid biases.\n",
    "                            if random.random() < 0.5:\n",
    "                                del trunc_tokens[0]\n",
    "                            else:\n",
    "                                trunc_tokens.pop()\n",
    "\n",
    "                    truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\n",
    "\n",
    "                    assert len(tokens_a) >= 1\n",
    "                    assert len(tokens_b) >= 1\n",
    "\n",
    "                    # add special tokens\n",
    "                    input_ids = self.tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)\n",
    "                    # add token type ids, 0 for sentence a, 1 for sentence b\n",
    "                    token_type_ids = self.tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)\n",
    "                    \n",
    "                    # 드디어 아래 항목에 대한 데이터셋이 만들어졌습니다! :-)\n",
    "                    # 즉, segmentA[SEP]segmentB, [0, 0, .., 0, 1, 1, ..., 1], NSP 데이터가 만들어진 것입니다 :-)\n",
    "                    # 그럼 다음은.. 이 데이터에 [MASK] 를 씌워야겠죠?\n",
    "                    example = {\n",
    "                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                        \"next_sentence_label\": torch.tensor(1 if is_random_next else 0, dtype=torch.long),\n",
    "                    }\n",
    "\n",
    "                    self.examples.append(example)\n",
    "\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xzxapumQZtzY",
    "outputId": "51279160-080b-437f-dacf-3ecc6a097007"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (137 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dataset = TextDatasetForNextSentencePrediction(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='/content/my_data/wiki_20190620_small.txt',\n",
    "    block_size=128,\n",
    "    overwrite_cache=True,\n",
    "    short_seq_probability=0.1,\n",
    "    nsp_probability=0.5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSvqhdMDqn6R",
    "outputId": "a9b61c1a-79da-4640-dbbd-1a2ce0e6351d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2,  4354,   639,     5,  5497,     5,  5504, 11105,  2492,  2428,\n",
      "         2780,  1968,  5379,  3120,  1940,  2407,    16,  5497, 10307, 16248,\n",
      "          552,  1266,   822,  1024,  1207,   931, 16491, 12283,  1095,  3667,\n",
      "           16,  6532,  8934,  1077,  2677,  1906,    16,     3,   727,  1052,\n",
      "           93,  7742,    93, 10411,  1007, 18365,  3483, 18887,    16,  6439,\n",
      "         1968,  4021,   279,  3361,   657,  1401,  2105,  1933, 17664,    93,\n",
      "          439,  1114,  2137,     1,  2023,  4086, 17983,    16,  2062,   496,\n",
      "         2734,     5, 17664, 12973,     5,   379,  7718,    16,  4186,  6532,\n",
      "          750,   541,  1018,  2795,  4860,  5152,  4776,   176, 11840, 15561,\n",
      "          654,  2786,  9394,  1946,  2370,  2895,  2054,    14,  9869,  6532,\n",
      "          750,   762,  1116,  6463,  5152,  2823,  3951,  6532,   750,   762,\n",
      "         2092,  6208,  1899,    16,  2829,  4530,   727, 16248, 18217,  2661,\n",
      "        17631, 13570,  2489,    14,     3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1]), 'next_sentence_label': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "for example in dataset.examples[0:1]:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "YSmzT_Y8d_yE"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbowPIDCFuhF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybkZjqE1gUlt",
    "outputId": "06a1a6bd-6abc-47bc-a358-874031f67d7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2,   757,  3609,  1907,  3751,  4410,  2049,  2944,  2178,  1896,\n",
      "         1988,  3610,  2967,  1885,  7858,  1118,  2061,    16,  3954,  4326,\n",
      "         1227, 11705,  2189,  1015,  7815, 13051,  1987,  9863,    16,   757,\n",
      "           14, 17896, 15385, 14780, 12216, 11234,  6428,  1988,  2412, 10610,\n",
      "         1022, 12379,  1991,  5982,  9863,    16,     3, 17259,   705, 17869,\n",
      "         1903,  7274, 14064,  3013,  9005,  7578,  2025, 15151,  3296,  1261,\n",
      "         2758,  1897,    16,  9005,  9642, 17252,  1027,  5614,    14,  2418,\n",
      "         2886,  4867, 19579,  1903,  2713,  3123,  4846,    16,  9005,  1094,\n",
      "         8604,  3184,   705,  3662,  4060,  1976,  2115,  2454,  2301,  8025,\n",
      "        10911,  9601,  8604, 10848,  5920,    16,     3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1]), 'next_sentence_label': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "for example in dataset.examples[-2:-1]:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "1aZHJl0wgcjs"
   },
   "outputs": [],
   "source": [
    "mlm_data=data_collator(dataset.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y74bzR-F0uHG",
    "outputId": "0743da5b-083b-4549-a0fe-2e194b93d9cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,     4,   639,  ...,  2489,    14,     3],\n",
       "        [    2,  2761,     4,  ...,  5057,    16,     3],\n",
       "        [    2,  5961, 16532,  ...,     4,  2286,     3],\n",
       "        ...,\n",
       "        [    2, 16634,  2036,  ...,    22,  2651,     3],\n",
       "        [    2,   757,  3609,  ...,     0,     0,     0],\n",
       "        [    2,  7001,     4,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'next_sentence_label': tensor([0, 1, 1,  ..., 1, 1, 0]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ -100,  4354,  -100,  ...,  -100,  -100,  -100],\n",
       "        [ -100,  -100, 10313,  ...,  -100,  -100,  -100],\n",
       "        [ -100,  -100,  -100,  ..., 10174,  -100,  -100],\n",
       "        ...,\n",
       "        [ -100,  2055,  -100,  ...,  -100,  -100,  -100],\n",
       "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
       "        [ -100,  -100,  1930,  ...,  -100,  -100,  -100]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rcFuVTyvjx6",
    "outputId": "688d97e0-6f24-446e-bf0b-7ba0930c4810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    2,  4354,   639,     5,  5497,     5,  5504, 11105,  2492,  2428,\n",
      "         2780,  1968,  5379,  3120,  1940,  2407,    16,  5497,     4, 16248,\n",
      "          552,  1266,   822,  1024,  1207,   931, 16491, 12283,  1095,  3667,\n",
      "           16,  6532,  8934,  1077,  2677,  1906,    16,     3,   727,  1052,\n",
      "           93,  7742,    93, 10411,  1007, 18365,  3483, 18887,    16,  6439,\n",
      "         1968,  4021,   279,  3361,     4,  1401,  2105,  1933, 17664,    93,\n",
      "          439,  1114, 19303,     1,  2023,  4086, 17983,    16,  2062,   496,\n",
      "         2734,     5, 17664, 12973,     5,   379,  7718,    16,  4186,     4,\n",
      "          750,   541,   711,  2795,  4860,  5152,  4776,   176,     4, 15561,\n",
      "          654,  2786,     4,  1946,     4,  2895,  2054,    14,  9869,  6532,\n",
      "          750,   762,  1116,  6463,  5152,  2823,  3951,  6532,   750,   762,\n",
      "         2092,     4,  1899,    16,  2829,  4530,   727,  4783, 18217,  2661,\n",
      "        17631, 13570,  2489,    14,     3])\n"
     ]
    }
   ],
   "source": [
    "print(data_collator(dataset.examples)['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2qiaV75SgkVU",
    "outputId": "756e6cf7-a368-44f3-a0e6-854b18d4877d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  1968,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  1077,  2677,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,    16,  -100,\n",
      "         -100,  -100,  -100,  3361,  -100,  1401,  2105,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,    16,  -100,   496,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  6532,\n",
      "         -100,  -100,  1018,  -100,  4860,  -100,  -100,  -100, 11840,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  2895,  2054,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         2092,  -100,  1899,  -100,  2829,  -100,   727,  -100, 18217,  -100,\n",
      "         -100,  -100,  2489,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "print(data_collator(dataset.examples)['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "5fMgMNBmg_Ce",
    "outputId": "f1c2c8d6-49c6-44c7-8150-c5c5c0c30475"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] [MASK] 얼 \" 지미 \" 카터 상계 [MASK] 민주당 출신 미국 39번째 대통령 이다. 지미 [MASK] 조지아주 섬터 카운티 플레인스 마을에서 [MASK]. 조지아 [MASK]를 [MASK] [MASK] [MASK] [SEP] 전함 [MASK] 원자력 · 잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩 · 면화 등을 [UNK] 많은 돈을 벌었다. 그의 별 [MASK] \" 땅콩 [MASK] \" [MASK] 알려졌다. 1962년 섭 [MASK] 선형의원 의원 선거에서 낙선 [MASK] 그 [MASK] 부정선거 전쟁이음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 [MASK] 지사를 역임했다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, [SEP]'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data_collator(dataset.examples)['input_ids'][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "kYdW6tWnhE6t"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='model_output',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_gpu_train_batch_size=16,\n",
    "    save_steps=1000, # step 수마다 모델을 저장\n",
    "    save_total_limit=2, # 마지막 두 모델 빼고 과거 모델은 삭제\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator, # 밥을 어떻게 떠먹여줄지\n",
    "    train_dataset=dataset # 밥이 뭔지\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "mavrUbPahUF9",
    "outputId": "372c2de1-5d0b-446e-9565-5716591f4483"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "***** Running training *****\n",
      "  Num examples = 2726\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 342\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='342' max='342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [342/342 01:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>9.733200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>9.315500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>9.185300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=342, training_loss=9.384090222810444, metrics={'train_runtime': 83.2172, 'train_samples_per_second': 65.515, 'train_steps_per_second': 4.11, 'total_flos': 352718194962000.0, 'train_loss': 9.384090222810444, 'epoch': 2.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() # wiki 전체 데이터로 학습 시, 1 epoch에 9시간 정도 소요됩니다!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mI10a97jwJdm"
   },
   "outputs": [],
   "source": [
    "a= 1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrRpsieDwMY9"
   },
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "BERT pre-training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
