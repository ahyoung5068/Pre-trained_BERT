{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"1DAkwPJsXbjI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"],"metadata":{"id":"Uvk-rI88FGeK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #### kobert 다운로드\n","# !pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"],"metadata":{"id":"OjLP-L1uIu0l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"yx8o6trvNyZO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from transformers import BertTokenizer, BertModel\n","import tqdm\n","import time\n","import numpy as np"],"metadata":{"id":"JQE0tizLXDiT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install sentencepiece"],"metadata":{"id":"lAI0xym6OHQr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import sentencepiece as spm"],"metadata":{"id":"LxHQJ1k7OIvc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from kobert_tokenizer import KoBERTTokenizer\n","# tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","# tokenizer.encode(\"한국어 모델을 공유합니다.\")"],"metadata":{"id":"nyGhA93AMcE1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import torch\n","# from transformers import BertModel\n","# model = BertModel.from_pretrained('skt/kobert-base-v1',output_hidden_states = True)\n","# text = \"한국어 모델을 공유합니다.\"\n","# inputs = tokenizer.batch_encode_plus([text])\n","# out = model(input_ids = torch.tensor(inputs['input_ids']),\n","#               attention_mask = torch.tensor(inputs['attention_mask']))\n","# out.pooler_output.shape"],"metadata":{"id":"LSBjqIy8MpAD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model.to('cuda')"],"metadata":{"id":"JzTnpizm_qWf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# max_len = 512"],"metadata":{"id":"GRXvtx3J_qTz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def tokenizer_(text, max_len, tokenizer):\n","#     encoded_dict = tokenizer.encode_plus(text = text,\n","#                                      add_special_tokens = True,\n","#                                      max_length = max_len,\n","#                                      padding = 'max_length',\n","#                                      return_attention_mask = True,\n","#                                      truncation = True)\n","\n","#     input_id = encoded_dict['input_ids']\n","#     token_type_id = encoded_dict['token_type_ids']\n","#     attention_mask = encoded_dict['attention_mask']\n","\n","#     return input_id, token_type_id, attention_mask"],"metadata":{"id":"RrKZ4nGg_qRO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def bert_embedding(text):\n","#     encoded_dict = tokenizer.encode_plus(text = text,\n","#                                      add_special_tokens = True,\n","#                                      max_length = max_len,\n","#                                      padding = 'max_length',\n","#                                      return_attention_mask = True,\n","#                                      truncation = True)\n","\n","#     tokens_tensor = torch.tensor([encoded_dict['input_ids']]).to('cuda')\n","#     segment_tensors = torch.tensor([encoded_dict['token_type_ids']]).to('cuda')\n","#     attention_tensors = torch.tensor([encoded_dict['attention_mask']]).to('cuda')\n","\n","#     model.eval()\n","\n","#     with torch.no_grad():\n","#         outputs = model(tokens_tensor, attention_tensors, segment_tensors)\n","\n","#     hidden_states = outputs[2]\n","\n","#     token_mean = []\n","\n","#     for h in hidden_states[-4:]:\n","#         token_mean.append(torch.mean(h[0], dim=0))\n","\n","#     last_four_sentence_embedding = sum(token_mean)\n","\n","#     return last_four_sentence_embedding.cpu().numpy()"],"metadata":{"id":"jb_vs9Cm_qOw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df = pd.read_excel('/content/drive/MyDrive/아영/제주학회/관세용어사전_길이순.xlsx', index_col = 0)"],"metadata":{"id":"Tlju5SMq_qL8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df"],"metadata":{"id":"qDZ5UJf6_qJJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# text = df[\"단어 설명\"][0]"],"metadata":{"id":"YEX7bSE5_qGc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# text"],"metadata":{"id":"wjqfSd4DvS1o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenizer(\n","#     text,\n","#     return_token_type_ids=False,\n","#     return_attention_mask=False\n","# )"],"metadata":{"id":"bWVVqX9X_qD-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(tokenizer.convert_ids_to_tokens([2, 3803, 6018, 6424, 6733, 7003, 7178, 5379, 4541, 6745, 5418, 6307, 7095, 867, 6903, 2900, 7148, 2900, 6855, 7089, 5808, 5557, 947, 7789, 4012, 7104, 2734, 7202, 6553, 6107, 6116, 3765, 6291, 3820, 7828, 2076, 6896, 4955, 7815, 4467, 7836, 2872, 3866, 4930, 1751, 2254, 7087, 7086, 1185, 5112, 6241, 6896, 1682, 1612, 5524, 5468, 2844, 5525, 3592, 7794, 4137, 3023, 7178, 6896, 517, 7095, 5377, 7815, 2254, 5899, 5112, 7946, 6855, 7089, 7086, 2900, 7148, 1185, 3220, 7089, 7095, 1633, 5557, 4302, 7819, 3149, 5760, 1185, 3220, 7089, 7095, 1612, 6079, 517, 7428, 6398, 5894, 3860, 2734, 7202, 6553, 6107, 6116, 3836, 6629, 7836, 2872, 3270, 7089, 2095, 6733, 7003, 7178, 7095, 3220, 7089, 5859, 1185, 3220, 7089, 7095, 1633, 5557, 4302, 7819, 3149, 6554, 2734, 7202, 6553, 6107, 6116, 3836, 6629, 7836, 2872, 3270, 7089, 1864, 3023, 7178, 7095, 3574, 6228, 6983, 1076, 6883, 947, 7149, 7088, 3605, 7079, 4402, 7138, 7815, 2900, 5808, 5557, 947, 7794, 968, 6901, 947, 4012, 5592, 5760, 2734, 7202, 6553, 6107, 5760, 3605, 7095, 1612, 6241, 2900, 7150, 2734, 7202, 6553, 6107, 5330, 3283, 5760, 2900, 7636, 5474, 6733, 5439, 6116, 4977, 2872, 5330, 3271, 1864, 3516, 7119, 7079, 2900, 7941, 6242, 3765, 2233, 2872, 3278, 867, 6527, 2499, 7096, 1951, 1836, 2734, 7202, 6553, 6107, 7095, 2844, 5524, 7086, 1612, 6241, 6081, 1185, 5398, 7088, 2384, 3860, 3605, 6896, 3864, 911, 3785, 7788, 1185, 2734, 7202, 6553, 6107, 6116, 1633, 5859, 6288, 5561, 3552, 7815, 3605, 6607, 7227, 7095, 2900, 7941, 6241, 5808, 5859, 6736, 6553, 6896, 4919, 6513, 7088, 1258, 7191, 7788, 3605, 6896, 4156, 7837, 3605, 7086, 2900, 7941, 6241, 5808, 5859, 6983, 2688, 7078, 2734, 7202, 6553, 6107, 6116, 2900, 7164, 1633, 5859, 7837, 4279, 2900, 7941, 6241, 5808, 5859, 5760, 3605, 7086, 1612, 5525, 5126, 7828, 4451, 6079, 2900, 7164, 1612, 7941, 6242, 1633, 5859, 7788, 2900, 7150, 5112, 6241, 6168, 5336, 5808, 5550, 7078, 1633, 5550, 5418, 1862, 4402, 7138, 5557, 2688, 7794, 4137, 3]))"],"metadata":{"id":"IE-6hNH4_qBZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6PzkwxkO_p-W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# text2 = \"나는 밥을 먹는다\""],"metadata":{"id":"TFnI9ca5_p7s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenizer(\n","#     text2,\n","#     return_token_type_ids=False,\n","#     return_attention_mask=False\n","# )"],"metadata":{"id":"hnBB5YV2_p5M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(tokenizer.convert_ids_to_tokens([2, 1375, 2266, 7088, 2010, 5760, 5782, 3]))"],"metadata":{"id":"Q5HebkJo_p2x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"R4PbYshS_p0S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JxUBQv6l_pxl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UhwQBRX2_pvH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#KoBERT (monologg/kobert)"],"metadata":{"id":"Ve3603ujvrhv"}},{"cell_type":"code","source":["# # Import generic wrappers\n","# from transformers import AutoModel, AutoTokenizer\n","\n","\n","# # Define the model repo\n","# model_name = \"monologg/kobert\"\n","\n","\n","# # Download pytorch model\n","# model = AutoModel.from_pretrained(model_name)\n","# tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","\n","# # Transform input tokens\n","# inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n","\n","# # Model apply\n","# outputs = model(**inputs)"],"metadata":{"id":"OrNs4e2-gliu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MdP7bJgHkBjZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# coding=utf-8\n","# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team and Jangwon Park\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\" Tokenization classes for KoBERT model \"\"\"\n","\n","\n","import logging\n","import os\n","import unicodedata\n","from shutil import copyfile\n","\n","from transformers import PreTrainedTokenizer\n","\n","logger = logging.getLogger(__name__)\n","\n","VOCAB_FILES_NAMES = {\n","    \"vocab_file\": \"tokenizer_78b3253a26.model\",\n","    \"vocab_txt\": \"vocab.txt\",\n","}\n","\n","PRETRAINED_VOCAB_FILES_MAP = {\n","    \"vocab_file\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\",\n","    },\n","    \"vocab_txt\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\",\n","    },\n","}\n","\n","PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n","    \"monologg/kobert\": 512,\n","    \"monologg/kobert-lm\": 512,\n","    \"monologg/distilkobert\": 512,\n","}\n","\n","PRETRAINED_INIT_CONFIGURATION = {\n","    \"monologg/kobert\": {\"do_lower_case\": False},\n","    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n","    \"monologg/distilkobert\": {\"do_lower_case\": False},\n","}\n","\n","SPIECE_UNDERLINE = \"▁\"\n","\n","\n","class KoBertTokenizer(PreTrainedTokenizer):\n","    \"\"\"\n","    SentencePiece based tokenizer. Peculiarities:\n","        - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n","    \"\"\"\n","\n","    vocab_files_names = VOCAB_FILES_NAMES\n","    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n","    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n","    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n","\n","    def __init__(\n","        self,\n","        vocab_file,\n","        vocab_txt,\n","        do_lower_case=False,\n","        remove_space=True,\n","        keep_accents=False,\n","        unk_token=\"[UNK]\",\n","        sep_token=\"[SEP]\",\n","        pad_token=\"[PAD]\",\n","        cls_token=\"[CLS]\",\n","        mask_token=\"[MASK]\",\n","        **kwargs,\n","    ):\n","        super().__init__(\n","            unk_token=unk_token,\n","            sep_token=sep_token,\n","            pad_token=pad_token,\n","            cls_token=cls_token,\n","            mask_token=mask_token,\n","            **kwargs,\n","        )\n","\n","        # Build vocab\n","        self.token2idx = dict()\n","        self.idx2token = []\n","        with open(vocab_txt, \"r\", encoding=\"utf-8\") as f:\n","            for idx, token in enumerate(f):\n","                token = token.strip()\n","                self.token2idx[token] = idx\n","                self.idx2token.append(token)\n","\n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\n","                \"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                \"pip install sentencepiece\"\n","            )\n","\n","        self.do_lower_case = do_lower_case\n","        self.remove_space = remove_space\n","        self.keep_accents = keep_accents\n","        self.vocab_file = vocab_file\n","        self.vocab_txt = vocab_txt\n","\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(vocab_file)\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.idx2token)\n","\n","    def get_vocab(self):\n","        return dict(self.token2idx, **self.added_tokens_encoder)\n","\n","    def __getstate__(self):\n","        state = self.__dict__.copy()\n","        state[\"sp_model\"] = None\n","        return state\n","\n","    def __setstate__(self, d):\n","        self.__dict__ = d\n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\n","                \"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                \"pip install sentencepiece\"\n","            )\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(self.vocab_file)\n","\n","    def preprocess_text(self, inputs):\n","        if self.remove_space:\n","            outputs = \" \".join(inputs.strip().split())\n","        else:\n","            outputs = inputs\n","        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n","\n","        if not self.keep_accents:\n","            outputs = unicodedata.normalize(\"NFKD\", outputs)\n","            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n","        if self.do_lower_case:\n","            outputs = outputs.lower()\n","\n","        return outputs\n","\n","    def _tokenize(self, text):\n","        \"\"\"Tokenize a string.\"\"\"\n","        text = self.preprocess_text(text)\n","        pieces = self.sp_model.encode(text, out_type=str)\n","        new_pieces = []\n","        for piece in pieces:\n","            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n","                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n","                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n","                    if len(cur_pieces[0]) == 1:\n","                        cur_pieces = cur_pieces[1:]\n","                    else:\n","                        cur_pieces[0] = cur_pieces[0][1:]\n","                cur_pieces.append(piece[-1])\n","                new_pieces.extend(cur_pieces)\n","            else:\n","                new_pieces.append(piece)\n","\n","        return new_pieces\n","\n","    def _convert_token_to_id(self, token):\n","        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n","        return self.token2idx.get(token, self.token2idx[self.unk_token])\n","\n","    def _convert_id_to_token(self, index):\n","        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n","        return self.idx2token[index]\n","\n","    def convert_tokens_to_string(self, tokens):\n","        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n","        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n","        return out_string\n","\n","    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n","        by concatenating and adding special tokens.\n","        A KoBERT sequence has the following format:\n","            single sequence: [CLS] X [SEP]\n","            pair of sequences: [CLS] A [SEP] B [SEP]\n","        \"\"\"\n","        if token_ids_1 is None:\n","            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        sep = [self.sep_token_id]\n","        return cls + token_ids_0 + sep + token_ids_1 + sep\n","\n","    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n","        \"\"\"\n","        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n","        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n","        Args:\n","            token_ids_0: list of ids (must not contain special tokens)\n","            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n","                for sequence pairs\n","            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n","                special tokens for the model\n","        Returns:\n","            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n","        \"\"\"\n","\n","        if already_has_special_tokens:\n","            if token_ids_1 is not None:\n","                raise ValueError(\n","                    \"You should not supply a second sequence if the provided sequence of \"\n","                    \"ids is already formated with special tokens for the model.\"\n","                )\n","            return list(\n","                map(\n","                    lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0,\n","                    token_ids_0,\n","                )\n","            )\n","\n","        if token_ids_1 is not None:\n","            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n","        return [1] + ([0] * len(token_ids_0)) + [1]\n","\n","    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n","        A KoBERT sequence pair mask has the following format:\n","        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n","        | first sequence    | second sequence\n","        if token_ids_1 is None, only returns the first portion of the mask (0's).\n","        \"\"\"\n","        sep = [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        if token_ids_1 is None:\n","            return len(cls + token_ids_0 + sep) * [0]\n","        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n","\n","    def save_vocabulary(self, save_directory):\n","        \"\"\"Save the sentencepiece vocabulary (copy original file) and special tokens file\n","        to a directory.\n","        \"\"\"\n","        if not os.path.isdir(save_directory):\n","            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n","            return\n","\n","        # 1. Save sentencepiece model\n","        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n","\n","        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n","            copyfile(self.vocab_file, out_vocab_model)\n","\n","        # 2. Save vocab.txt\n","        index = 0\n","        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n","        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n","            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n","                if index != token_index:\n","                    logger.warning(\n","                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n","                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n","                    )\n","                    index = token_index\n","                writer.write(token + \"\\n\")\n","                index += 1\n","\n","        return out_vocab_model, out_vocab_txt"],"metadata":{"id":"lsQkN_JAqp3X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# kobert 토크나이즈를 임포트합니다.\n","tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"],"metadata":{"id":"dg1klnWbp4q1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 버트를 사용하기에 앞서 가장 기초에 속하는 tokenizer 사용 방법에 대해서 잠시 배워보도록 하겠습니다.\n","# tokenizer.encode => 문장을 버트 모델의 인풋 토큰값으로 바꿔줌\n","# tokenizer.tokenize => 문장을 토큰화\n","print(tokenizer.encode(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))"],"metadata":{"id":"SCUFRQCxqEWh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertModel\n","model = BertModel.from_pretrained('monologg/kobert')"],"metadata":{"id":"tjbhYuatnY0b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from transformers import BertModel, DistilBertModel\n","# model = BertModel.from_pretrained(\"monologg/kobert\")"],"metadata":{"id":"GRokCaNakCJH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZPlvHJTGkCGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.to('cuda')"],"metadata":{"id":"SlTnFLk_glgf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = 512"],"metadata":{"id":"EYBVzAaZgleC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenizer_(text, max_len, tokenizer):\n","    encoded_dict = tokenizer.encode_plus(text = text,\n","                                     add_special_tokens = True,\n","                                     max_length = max_len,\n","                                     padding = 'max_length',\n","                                     return_attention_mask = True,\n","                                     truncation = True)\n","\n","    input_id = encoded_dict['input_ids']\n","    token_type_id = encoded_dict['token_type_ids']\n","    attention_mask = encoded_dict['attention_mask']\n","\n","    return input_id, token_type_id, attention_mask"],"metadata":{"id":"9QXEESVbglbz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def bert_embedding(text):\n","    encoded_dict = tokenizer.encode_plus(text = text,\n","                                     add_special_tokens = True,\n","                                     max_length = max_len,\n","                                     padding = 'max_length',\n","                                     return_attention_mask = True,\n","                                     truncation = True)\n","\n","    tokens_tensor = torch.tensor([encoded_dict['input_ids']]).to('cuda')\n","    segment_tensors = torch.tensor([encoded_dict['token_type_ids']]).to('cuda')\n","    attention_tensors = torch.tensor([encoded_dict['attention_mask']]).to('cuda')\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor, attention_tensors, segment_tensors)\n","\n","    hidden_states = outputs[2]\n","\n","    token_mean = []\n","\n","    for h in hidden_states[-4:]:\n","        token_mean.append(torch.mean(h[0], dim=0))\n","\n","    last_four_sentence_embedding = sum(token_mean)\n","\n","    return last_four_sentence_embedding.cpu().numpy()"],"metadata":{"id":"2DnV7bpIglZP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"5rJ-EN9atyYu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_excel('/content/drive/MyDrive/제주학회/관세용어사전_길이순.xlsx', index_col = 0)"],"metadata":{"id":"n2uFmizntySF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = df[\"단어 설명\"][0]"],"metadata":{"id":"RNlYZ9hF0IgP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text"],"metadata":{"id":"q-PclRBFvnMq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer(\n","    text,\n","    return_token_type_ids=False,\n","    return_attention_mask=False\n",")"],"metadata":{"id":"ISTqLi-O0Idr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.convert_ids_to_tokens([2, 3803, 6018, 6424, 6733, 7003, 7178, 5379, 4541, 6745, 5418, 6307, 7095, 867, 6903, 2900, 7148, 2900, 6855, 7089, 5808, 5557, 947, 7789, 4012, 7104, 2734, 7202, 6553, 6107, 6116, 3765, 6291, 3820, 7828, 2076, 6896, 4955, 7815, 4467, 7836, 2872, 3866, 4930, 1751, 2254, 7087, 7086, 1185, 5112, 6241, 6896, 1682, 1612, 5524, 5468, 2844, 5525, 3592, 7794, 4137, 3023, 7178, 6896, 517, 7095, 5377, 7815, 2254, 5899, 5112, 7946, 6855, 7089, 7086, 2900, 7148, 1185, 3220, 7089, 7095, 1633, 5557, 4302, 7819, 3149, 5760, 1185, 3220, 7089, 7095, 1612, 6079, 517, 7428, 6398, 5894, 3860, 2734, 7202, 6553, 6107, 6116, 3836, 6629, 7836, 2872, 3270, 7089, 2095, 6733, 7003, 7178, 7095, 3220, 7089, 5859, 1185, 3220, 7089, 7095, 1633, 5557, 4302, 7819, 3149, 6554, 2734, 7202, 6553, 6107, 6116, 3836, 6629, 7836, 2872, 3270, 7089, 1864, 3023, 7178, 7095, 3574, 6228, 6983, 1076, 6883, 947, 7149, 7088, 3605, 7079, 4402, 7138, 7815, 2900, 5808, 5557, 947, 7794, 968, 6901, 947, 4012, 5592, 5760, 2734, 7202, 6553, 6107, 5760, 3605, 7095, 1612, 6241, 2900, 7150, 2734, 7202, 6553, 6107, 5330, 3283, 5760, 2900, 7636, 5474, 6733, 5439, 6116, 4977, 2872, 5330, 3271, 1864, 3516, 7119, 7079, 2900, 7941, 6242, 3765, 2233, 2872, 3278, 867, 6527, 2499, 7096, 1951, 1836, 2734, 7202, 6553, 6107, 7095, 2844, 5524, 7086, 1612, 6241, 6081, 1185, 5398, 7088, 2384, 3860, 3605, 6896, 3864, 911, 3785, 7788, 1185, 2734, 7202, 6553, 6107, 6116, 1633, 5859, 6288, 5561, 3552, 7815, 3605, 6607, 7227, 7095, 2900, 7941, 6241, 5808, 5859, 6736, 6553, 6896, 4919, 6513, 7088, 1258, 7191, 7788, 3605, 6896, 4156, 7837, 3605, 7086, 2900, 7941, 6241, 5808, 5859, 6983, 2688, 7078, 2734, 7202, 6553, 6107, 6116, 2900, 7164, 1633, 5859, 7837, 4279, 2900, 7941, 6241, 5808, 5859, 5760, 3605, 7086, 1612, 5525, 5126, 7828, 4451, 6079, 2900, 7164, 1612, 7941, 6242, 1633, 5859, 7788, 2900, 7150, 5112, 6241, 6168, 5336, 5808, 5550, 7078, 1633, 5550, 5418, 1862, 4402, 7138, 5557, 2688, 7794, 4137, 3]))"],"metadata":{"id":"ZZ7bmh8a0IYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch"],"metadata":{"id":"LFH8yLo4wrE8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KfegF62419wC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text2 = \"나는 밥을 먹는다\""],"metadata":{"id":"5MoHhk1OyOvq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer(\n","    text2,\n","    return_token_type_ids=False,\n","    return_attention_mask=False\n",")"],"metadata":{"id":"fNcDkLJtyOqT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.convert_ids_to_tokens([2, 1375, 2266, 7088, 2010, 5760, 5782, 3]))"],"metadata":{"id":"2lBBl52FyT3K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AWiWHjfnyjfF"},"execution_count":null,"outputs":[]}]}